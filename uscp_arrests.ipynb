{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/golfecholima/uscp_arrests/blob/master/uscp_arrests.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j-k3Crh3X90F"
   },
   "source": [
    "# Downloading and parsing US Capitol Police arrest reports\n",
    "\n",
    "Thanks to Mike Stucka for this great tutorial\n",
    "[this great tutorial on scraping with pyquery](https://github.com/PalmBeachPost/nicar19scraping/blob/master/00-Scraping%20--%20full%20self-tutorial.ipynb) ... yeah, I switched to bs4 but this got me started.\n",
    "\n",
    "### Things used:\n",
    "_Install with `pip3 install -r requirements.txt`_\n",
    "\n",
    "* [requests](https://2.python-requests.org/en/master/)\n",
    "* [pdfplumber](https://github.com/jsvine/pdfplumber)\n",
    "* [pandas](https://pandas.pydata.org)\n",
    "* [beautifulsoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "* [datasette](https://datasette.io)\n",
    "* [csvs-to-sqlite](https://github.com/simonw/csvs-to-sqlite)\n",
    "* Logger from [@ilanamarcus](https://github.com/ilanamarcus)\n",
    "\n",
    "### Here's where the arrest reports [live](https://www.uscp.gov/media-center/weekly-arrest-summary).\n",
    "\n",
    "### To do:\n",
    "* ~~Refine datasette:~~\n",
    "    * ~~SQLite apparently infers that number an int, should stay string in case of leading zeros. (This apparently might not be possible.) Fixed w/ --shape.~~\n",
    "* ~~Functionify dir creation~~\n",
    "* ~~Make a new csv each time the script runs~~\n",
    "* ~~requirements.txt~~\n",
    "    * Include ipynb in requirements\n",
    "* Implement emails w/ function\n",
    "* YAML/cron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8m9qVmGOVMIm"
   },
   "outputs": [],
   "source": [
    "# External dependencies\n",
    "import requests\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from Logger import Log\n",
    "\n",
    "# Built-in dependencies\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import datetime\n",
    "import subprocess\n",
    "import urllib\n",
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError\n",
    "from urllib.error import URLError\n",
    "\n",
    "# Set up logging\n",
    "log = Log().getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P6o0-Auap8f-"
   },
   "outputs": [],
   "source": [
    "base_url = 'https://www.uscp.gov'\n",
    "url = base_url + '/media-center/weekly-arrest-summary'\n",
    "dt = str(datetime.datetime.now().strftime('%Y%m%d_%H%M%S'))\n",
    "wd = os.getcwd()\n",
    "\n",
    "# Making directories\n",
    "def mkdir(d):\n",
    "    log.debug(f'Creating the directory {d} ...')\n",
    "    \n",
    "    if os.path.isdir(d):\n",
    "        log.debug(f'Directory already exists.')\n",
    "    else:\n",
    "        try:\n",
    "            os.mkdir(d)\n",
    "        except OSError as e:\n",
    "            log.error(e)\n",
    "            log.debug(f'Failed.')\n",
    "        else:\n",
    "            log.debug(f'Success.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v1inO1q7zNYR"
   },
   "outputs": [],
   "source": [
    "# Get the HTML and download the PDFs \n",
    "def download(site):\n",
    "    \n",
    "    reports = wd + '/reports'\n",
    "    \n",
    "    mkdir(reports)\n",
    "    \n",
    "    try:\n",
    "        log.debug(f'Getting {site}.')\n",
    "        html = urlopen(site)\n",
    "    except HTTPError as e:\n",
    "        log.error(e)\n",
    "        log.debug('Failed.')\n",
    "        # Send a message somewhere\n",
    "    except URLError as e:\n",
    "        log.error(e)\n",
    "        log.debug('Failed.')\n",
    "        # Send a message somewhere\n",
    "    else:\n",
    "        log.debug('Done.')\n",
    "    \n",
    "    bs = BeautifulSoup(html.read(), 'html.parser')\n",
    "    links = bs.find_all('a', text= re.compile('Arrest Summary .+'))\n",
    "    \n",
    "    if not links:\n",
    "        log.debug('No links found. Hmmm, maybe the URL changed ...')\n",
    "        # This because bad URL slug ending still returned a page, just not the right one\n",
    "        # Send a message somewhere\n",
    "    else:\n",
    "        for link in links:\n",
    "            try:\n",
    "                href = link.attrs['href']\n",
    "            except AttributeError as e:\n",
    "                log.error(e)\n",
    "                log.debug('Failed.')\n",
    "                # Send a message somewhere\n",
    "            else:\n",
    "                filename = '/' + href.rsplit('/', 1)[1].lower().replace('%20', '_')\n",
    "                log.debug('Downloading ' + filename)\n",
    "                urllib.request.urlretrieve(base_url + href, reports + filename)\n",
    "    \n",
    "    pdfs = glob.glob(reports + '/*')\n",
    "    return pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3U4YBhiXp1ZR"
   },
   "outputs": [],
   "source": [
    "# Parse the downloaded PDFs\n",
    "def parse_pdf(pdfs):\n",
    "    log.debug('Parsing the PDFs ...')\n",
    "    \n",
    "    rows = []\n",
    "    \n",
    "    for pdf in pdfs:\n",
    "        plumb = pdfplumber.open(pdf)\n",
    "        pages = plumb.pages # A list of PDF page objects\n",
    "        pages_text = ''\n",
    "\n",
    "        for page in pages:\n",
    "            text = page.extract_text()\n",
    "            pages_text += text\n",
    "\n",
    "        pages_text = re.sub(r'(^\\d\\s*(\\n|$))', '\\n', pages_text, flags=re.M) # Get rid of the page numbers\n",
    "\n",
    "        # Regex to find each arrest report chunk https://regex101.com/r/kWkaLi/7\n",
    "        regex = (\n",
    "                r'((?:(?:.+\\n)(?=(?:(?:\\d{1,2}\\/\\d{1,2}\\/\\d{2,4})(?:\\s+)(?:\\d{1,2}:\\d{1,2})(?:\\s+)(?:\\d{5,12}))))'\n",
    "                r'(?:(?:\\d{1,2}\\/\\d{1,2}\\/\\d{2,4})(?:\\s+)(?:\\d{1,2}:\\d{1,2})(?:\\s+)(?:\\d{5,12}))'\n",
    "                r'(?:(?:[\\s\\S]+?(?=(?:\\Z)|(?:(?:(?:.+\\n)(?=(?:(?:\\d{1,2}\\/\\d{1,2}\\/\\d{2,4})(?:\\s+)(?:\\d{1,2}:\\d{1,2})(?:\\s+)(?:\\d{5,12})))))))))'\n",
    "        )\n",
    "\n",
    "        chunks = re.findall(regex, pages_text, flags=re.M)\n",
    "\n",
    "        for chunk in chunks:\n",
    "            rows.append(chunk)\n",
    "    \n",
    "    log.debug('Done.')\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5f9-PUB0zNYU",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Parse each chunk/row to cols then dataframe\n",
    "def parse_row(rows):\n",
    "    log.debug('Parsing each row into columns ...')\n",
    "    title = []\n",
    "    date = []\n",
    "    time = []\n",
    "    number = []\n",
    "    narrative = []\n",
    "    d = {\n",
    "    'title': title,\n",
    "    'date': date,\n",
    "    'time': time,\n",
    "    'number': number,\n",
    "    'narrative': narrative\n",
    "    }\n",
    "    \n",
    "    for row in rows:\n",
    "        row = row.strip() # Remove leading and trailing whitespace\n",
    "\n",
    "        # Regex to slice up the different data points of each 'chunk'\n",
    "        regex = r'(^.+\\n)(?:(\\d{1,2}\\/\\d{1,2}\\/\\d{2,4})(?:\\s+)(\\d{1,2}:\\d{1,2})(?:\\s+)(\\d{5,12}))([\\s\\S]+)'\n",
    "\n",
    "        titles = re.search(regex, row).group(1).strip()\n",
    "        dates = re.search(regex, row).group(2).strip()\n",
    "        times = re.search(regex, row).group(3).strip()\n",
    "        numbers = re.search(regex, row).group(4).strip()\n",
    "        narratives = re.sub('\\n', '',(re.search(regex, row).group(5).strip()))\n",
    "\n",
    "        title.append(titles)\n",
    "        date.append(dates)\n",
    "        time.append(times)\n",
    "        number.append(numbers)\n",
    "        narrative.append(narratives)\n",
    "    \n",
    "    log.debug('... putting into dataframe ...')\n",
    "    \n",
    "    df = pd.DataFrame(data = d)\n",
    "    \n",
    "    df['datetime'] = df['date'].map(str) + ' ' + df['time'] # Merge the date and time columns\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'], infer_datetime_format = True) # Make that new column a datetime type\n",
    "    df['date'] = df['datetime'].dt.date # Split off date\n",
    "    df['time'] = df['datetime'].dt.time # Split off time\n",
    "    \n",
    "    log.debug('Done.')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rd56g1voOejG"
   },
   "outputs": [],
   "source": [
    "# Make a CSV with datetime label that goes in the csv dir\n",
    "def mkcsv(df):\n",
    "    log.debug('Converting dataframe to csv ...')\n",
    "    csvs = wd + '/csv'\n",
    "    csv = csvs + '/uscp_arrests_' + dt + '.csv'\n",
    "    \n",
    "    mkdir(csvs)\n",
    "\n",
    "    df.to_csv(csv, encoding='utf-8', index=False)\n",
    "    log.debug(f'Saved the file {csv}.')\n",
    "    \n",
    "    return csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ud1PQ4L7_vCY"
   },
   "outputs": [],
   "source": [
    "# Put it all in a datasette and 'publish'\n",
    "def ds(csv):\n",
    "    log.debug(f'Converting {csv} to .db ...')\n",
    "    dbs = wd + '/db'\n",
    "    db = dbs + '/uscp_arrests.db'\n",
    "    \n",
    "    mkdir(dbs)\n",
    "    \n",
    "    # Running terminal commands from python\n",
    "    subprocess.check_call([\n",
    "        'csvs-to-sqlite',\n",
    "        '--replace-tables',\n",
    "        '--shape',\n",
    "        'title:title,date:date(TEXT),time:time(TEXT),number:number(TEXT),narrative:narrative,datetime:datetime(TEXT)',\n",
    "        csv,\n",
    "        db]) \n",
    "    # ^^ Trixy ^^ any time you would have a space in the command line\n",
    "    # you need to comma separate and have a news string in the brackets.\n",
    "\n",
    "    log.debug('Starting datasette at http://127.0.0.1:8001/uscp_arrests ...')\n",
    "    subprocess.check_call(['datasette', db])\n",
    "    \n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SYSxwfKHzNYZ"
   },
   "outputs": [],
   "source": [
    "# Do the things\n",
    "\n",
    "pdfs = download(url)\n",
    "rows = parse_pdf(pdfs)\n",
    "df = parse_row(rows)\n",
    "csv = mkcsv(df)\n",
    "ds(csv)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "uscp_arrests.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
